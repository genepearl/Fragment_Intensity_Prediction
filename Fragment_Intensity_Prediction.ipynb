{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf0y4ZaCg6HkEpxFkFSUl0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veni-vidi-vici-tech/Fragment_Intensity_Prediction/blob/main/Fragment_Intensity_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fragment Intensity Prediction"
      ],
      "metadata": {
        "id": "RvCP8daVK4ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GOAL**: Develop a machine learning model to predict Fragment ion intensities of y and b singly charged ions.\n",
        "\n",
        "Example:\n",
        "* Input: </br>\n",
        "Peptide Sequence: LTQETNRV, Precursor Charge: 4 \n",
        "* Output: \n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1CbuRh3qZB7p6c26AuEADXBoPCFD3qUXs' width=600>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oykTuUbeK_as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Data Analysis"
      ],
      "metadata": {
        "id": "08_7G9YAQnUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 74,817,546 initial entries\n",
        "\n",
        "* 7 columns with no missing values:\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1yZ-5BOeQx04ly01LEtisYDwGTlw9TxOm' width=200>\n",
        "\n",
        "\n",
        "###peptide_sequence column\n",
        "\n",
        "* 393.369 unique sequences\n",
        "* 7-30 amino acid length range\n",
        "<img src='https://drive.google.com/uc?id=1bs8JCiv5-uc-4MAcRh4NR2m66MUc8N1H' width=500>\n",
        "\n",
        "* max number of occurances: 31.516 (HWYITTGPVREK)\n",
        "* min number of occurances: 7 (QQQQQQQQQQQQRR)\n",
        "* average number of occurances per sequence: 190\n",
        "\n",
        "* There are two types of UNIMOD modifications: C[UNIMOD:4] and M[UNIMOD:35]\n",
        "* Cytosine is *always* UNIMOD:4 modified, while Methionine can be UNIMOD:35 modified or not\n",
        "\n",
        "###raw file and scan_number columns\n",
        "* One spectrum: raw_file and scan_number stay the same\n",
        "* **UNIQUE IDENTIFIER: peptide_sequence + raw_file + scan_number**\n",
        "\n",
        "###precursor charge\n",
        "\n",
        "  <img src='https://drive.google.com/uc?id=1WWu9ygJ2JtWQerhAuOMEXMkJUvhiYiOr' width=200>\n",
        "\n",
        "* The most common precursor_charge values are 2 and 3\n",
        "* Some sequences have several different precursor_charge values\n",
        "\n",
        "###ion type and no columns\n",
        "* There are two types of ions: b and y.\n",
        "* There are more y ions (37,404,506) than b ions (36,745,591)\n",
        "* 1-28 ion no range\n",
        "\n",
        "###intensity\n",
        " <img src= 'pictures_and_graphs/avarage_intensity.png' width=600>\n",
        "\n",
        "* values need to be normalized\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QDwX6gcEQpzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Data Preprocessing"
      ],
      "metadata": {
        "id": "AfI9H7o6Mi3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Removing Confusing Values\n",
        "\n",
        "- Data was merged from several files\n",
        "- There are 74,817,546 entries in our dataset at the beginning\n",
        "- **GOAL**: remove errors and noise from the data"
      ],
      "metadata": {
        "id": "nktxjbp5ZV_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "  ''' \n",
        "  Checks the DataFrame for errors and noise and \n",
        "  returns it without them\n",
        "  '''\n",
        "  # 2.1 Removing join errors\n",
        "  df['precursor_max'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file'])['precursor_charge'].transform(max)\n",
        "  df['precursor_min'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file'])['precursor_charge'].transform(min)\n",
        "  df.loc[df['precursor_max'] != df['precursor_min']]\n",
        "  df.drop(df.index[df['precursor_max'] != df['precursor_min']], inplace = True)\n",
        "  df.drop(['precursor_max', 'precursor_min'], axis=1, inplace=True)\n",
        "\n",
        "  # 2.2 Removing noise from the data\n",
        "  df.drop(df.index[df[\"peptide_sequence\"].str.len() == df['no']], inplace = True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "A9W6DtRIZwcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After applying clean_data function 667958 rows were removed from the dataset"
      ],
      "metadata": {
        "id": "K9x6cy5YZ6ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Peptide_Sequence Column Cleaning\n",
        "\n",
        "* `peptide_sequence` column contains UNIMOD modifications:\n",
        "  *  `C[UNIMOD:4]` = cystein is **always modified** (in carboxyamidomethylated form)\n",
        "  *  `M[UNIMOD:35]` = methionine is **in some cases modified** (oxidized to methionine sulfoxide) \n",
        "\n",
        "* **GOAL**: make the data in 'peptide_sequence' column consistent by removing [UNIMOD:4] and replacing [UNIMOD:35] with 'O'"
      ],
      "metadata": {
        "id": "390gU2B6Z5Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['peptide_sequence'] = df['peptide_sequence'].apply(lambda x: x.replace('[UNIMOD:4','')\n",
        "                                                                 .replace(']','')\n",
        "                                                                 .replace('M[UNIMOD:35', 'O'))"
      ],
      "metadata": {
        "id": "Cw1ZBSgwa-F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exploring intensities of neighbouring amino acids"
      ],
      "metadata": {
        "id": "Vb8ZtZWsitHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The `b and y ions` for a given peptide represent the two halves formed by splitting the original peptide at a **peptide bond** between two amino acids.\n",
        "\n",
        "1. Reversing the `peptide_sequence` for rows with `ion_type == y`, since y ions extend from the back (C-Terminus)\n",
        "2. Creating `neighbour_pair` column. Having the sequence, ion_type and no, we can find neighbouring amino acids (between which the fragmentation took place)\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-XJbDsu7sDvtnudbU0U0OS-l20tdp9b-\" width=\"600\"/>\n",
        "</div>\n",
        "3. Dividing `neighbour_pair` column into two columns. Since `b ions` extend from N-Terminus and `y ions` extend from C-Terminus, `c_neighbour` and `n_neighbour` columns are needed to use the neighbouring AA information from two different ion types meaningfully\n",
        "  1. Creating `c_neighbour` and `n_neighbour` columns columns\n",
        "  2. Filling two new columns for b ions, where the first element of `neighbour_pair` is `n_neighbour` and the second one is `c_neighbour`\n",
        "  3. Filling two new columns for y ions, where the first element of `neighbour_pair` is `c_neighbour` and the second one is `n_neighbour`\n",
        "4. Creating a `pivot_table`, where intensity values from all `neighbour pairs` are aggregated returning a median value for each pair (median and not mean is used, because intensity was normalized and there were no outliers)\n",
        "5. Plotting the `pivot_table` using heatmap function from Pythons Seaborn library"
      ],
      "metadata": {
        "id": "55wt4SQmhwg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1TVII1OdsERy4zyVM2zPRH5FMMcMAPwsG'>"
      ],
      "metadata": {
        "id": "M-gnugediLvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Proline** at *C-Terminus* tends to have high intensity with almost all neighboring amino acids\n",
        "* **Arginine** tends to have have intensity with neighboring Arginine"
      ],
      "metadata": {
        "id": "QKuXlxg_huMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Normalization\n",
        "\n",
        "- Big fluctuations between `intensity` values \n",
        "- **GOAL**: normalize `intensity` column values (values between 0.0 and 1.0)"
      ],
      "metadata": {
        "id": "qP-2BmSDbMdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_intensity(df):\n",
        "  '''\n",
        "  Takes intensity values from the DataFrame, normalizes them using \n",
        "  cumulative intensity normalization method and returns the DataFrame \n",
        "  with the new normalized values in a ['normalized_intensity'] column\n",
        "  '''\n",
        "  # 1. Assigning rank to every intensity in the spectra\n",
        "  df['rank'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file'])[\"intensity\"].rank(\"dense\", ascending=False)\n",
        "\n",
        "  # 2. Calculating intensity total within each spectra\n",
        "  df['total'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file'])[\"intensity\"].transform('sum')\n",
        "\n",
        "  # 3. Sorting values within the spectra based on their rank\n",
        "  df['sum_greater_than'] = df.sort_values('rank', ascending=False).groupby(['peptide_sequence', 'scan_number', 'raw_file'])['intensity'].cumsum()\n",
        "\n",
        "  # 4. Calculating the normalized intensity values within the spectra\n",
        "  df['normalized_intensity'] = df['sum_greater_than']/df['total']\n",
        "\n",
        "  # 5. Dropping helper-columns\n",
        "  df.drop(['rank', 'sum_greater_than', 'total'], axis = 1, inplace = True) \n",
        "  df\n",
        "  return df"
      ],
      "metadata": {
        "id": "mby5fZOib4fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1MkhKakBmWyPkT9VG7G-E7eCuzMK1JQ6_' width=600>"
      ],
      "metadata": {
        "id": "DVVSBIx-b9Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Turning data into numbers\n",
        "\n",
        "* `ion_type`, `no` and `intensity` are target features and need to be part of the same NumPy array structured like so:\n",
        "\n",
        "  * index 0-27 => ion type b 1-28\n",
        "  * index 28-55 => ion type y 1-28\n",
        "\n",
        "* `peptide_sequence` column needs to be converted into numbers using dictionary:\n",
        "\n",
        "  <img src=\"https://drive.google.com/uc?id=1If_VC9ENK1e_v0PFmxtLmtKtt_S7tqJ6\" \n",
        "  width=\"100\" /> \n"
      ],
      "metadata": {
        "id": "XFYMMKNAcR59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def create_target():\n",
        "  df['group'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file']).ngroup()\n",
        "  groups = df.groupby('group')\n",
        "  result = []\n",
        "  for name, group in groups:\n",
        "    intensities = np.zeros(56)\n",
        "    ion_groups = group.groupby('ion_type')\n",
        "\n",
        "    for ion_name, ion_group in ion_groups:\n",
        "      if ion_name == 'b':\n",
        "        indices = ion_group['no'] - 1\n",
        "        #intensities[indices] = ion_group['mean_normalized_intensity'].values\n",
        "        intensities[indices] = ion_group['normalized_intensity'].values\n",
        "      else:\n",
        "        indices = 27 + ion_group['no']\n",
        "        #intensities[indices] = ion_group['mean_normalized_intensity'].values\n",
        "        intensities[indices] = ion_group['normalized_intensity'].values\n",
        "    result.append(intensities)"
      ],
      "metadata": {
        "id": "7qxneXHic8PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 'peptide_length' column for further use\n",
        "df['peptide_length'] = df['peptide_sequence'].apply(len)"
      ],
      "metadata": {
        "id": "pUYbU9xHfHT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_peptides(df):\n",
        "\n",
        "  # 1. Make sure that all the sequences have the same length\n",
        "  # Finding the longest sequence\n",
        "  max_len = df['peptide_sequence'].str.len().max()\n",
        "  # Addding dummy values to shorter sequences to achieve the same length for every sequence\n",
        "  df['peptide_sequence_encoded'] = df['peptide_sequence'].apply(lambda x: x + 'X'*(max_len - len(x)))\n",
        "  # 2. Separate all the letters with comma\n",
        "  df['peptide_sequence_encoded'] = df['peptide_sequence_encoded'].agg(lambda x: ','.join(x))\n",
        "  # 3. Replace one-letter code with numbers\n",
        "  # Creating a dictionary, where dummy value X, every amino acid and 'O'(oxidized methionine) is numbered\n",
        "  aa_dict = {'X' : '0',\n",
        "             'A' : '1', 'C' : '2', 'D' : '3',\n",
        "             'E' : '4', 'F' : '5', 'G' : '6', \n",
        "             'H' : '7', 'I' : '8', 'K' : '9', \n",
        "             'L' : '10', 'M' : '11', 'N' : '12', \n",
        "             'P' : '13', 'Q' : '14', 'R' : '15',\n",
        "             'S' : '16', 'T' : '17', 'V' : '18',\n",
        "             'W' : '19', 'Y' : '20', 'O' : '21'}\n",
        "  # Iterating over all key-value pairs in aa_dict dictionary\n",
        "  for key, value in aa_dict.items():\n",
        "      # Replace key character(one letter code) with value character(number) in a sequence\n",
        "      df['peptide_sequence_encoded'] = df['peptide_sequence_encoded'].apply(lambda x: x.replace(key, value))\n",
        "\n",
        "  # 4. Turn string of numbers into integers\n",
        "  df['peptide_sequence_encoded'] = df['peptide_sequence_encoded'].apply(lambda x: [int(i) for i in x.split(\",\")])"
      ],
      "metadata": {
        "id": "aK5eG2CAdUR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Splitting the data\n",
        "\n",
        "* Some `peptide_sequence` values have several `precursor charge` values and need to be in the same split\n",
        "* **GOAL**: \n",
        "  * data split into 20% test_set + 80% training\n",
        "  * training further split into 5 cross-validation splits: 80% train + 20% validation\n",
        "  "
      ],
      "metadata": {
        "id": "S-6NNyhwdcXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "np.random.seed(19)\n",
        "def split_train_test(df):\n",
        "  '''\n",
        "  Splits the df into train and test splits\n",
        "  '''\n",
        "\n",
        "  # Create a new column in your dataframe that groups rows with the same peptide sequence value\n",
        "  df['peptide_group'] = df.groupby('peptide_sequence').ngroup()\n",
        "\n",
        "  # Create an instance of GroupShuffleSplit\n",
        "  gss = GroupShuffleSplit(n_splits=1, test_size=0.2)\n",
        "\n",
        "  # Split your dataframe into train and test sets using the iterator\n",
        "  for train_index, test_index in gss.split(df, groups=df['peptide_group']):\n",
        "      training = df.iloc[train_index]\n",
        "      test_df = df.iloc[test_index]\n",
        "\n",
        "  return training, test_df"
      ],
      "metadata": {
        "id": "FjXznECZeyQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_common_peptides(df_1, df_2):\n",
        "  '''\n",
        "  Finds common peptides between df_1 and df_2\n",
        "  '''\n",
        "  df_1_peptides = set(df_1['peptide_sequence'])\n",
        "  df_2_peptides = set(df_2['peptide_sequence'])\n",
        "\n",
        "  common_peptides = df_1_peptides & df_2_peptides\n",
        "  print(common_peptides)"
      ],
      "metadata": {
        "id": "xQEodk94foHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_precursor_percentage(splitted_df, original_df):\n",
        "  '''\n",
        "  Calculates the percentage of precursor charge values in the train split\n",
        "  '''\n",
        "  precursor_charge_counts = splitted_df['precursor_charge'].value_counts()\n",
        "  return ((precursor_charge_counts / original_df['precursor_charge'].value_counts()) * 100)\n",
        "\n",
        "\n",
        "def calculate_peptide_length_percentage(splitted_df, original_df):\n",
        "  '''\n",
        "  Calculates the percentage of precursor charge values in the train split\n",
        "  '''\n",
        "  peptide_length_counts = splitted_df['peptide_length'].value_counts()\n",
        "  return ((peptide_length_counts / original_df['peptide_length'].value_counts()) * 100)"
      ],
      "metadata": {
        "id": "o5w4g5cVfCag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_test(test_df):\n",
        "  '''\n",
        "  Transforms test_df into right format \n",
        "  and saves it as 3 NumPy arrays\n",
        "  '''\n",
        "  # Transforming data into numpy array\n",
        "  test_pre = test_df[[\"precursor_charge\"]].to_numpy()\n",
        "  test_int = test_df[[\"target\"]].to_numpy()\n",
        "  test_seq = test_df[[\"peptide_sequence_encoded\"]].to_numpy()\n",
        "\n",
        "  test_seq = np.array(test_seq).flatten()\n",
        "  test_int = np.array(test_int).flatten()\n",
        "  test_pre = test_pre.flatten()\n",
        "\n",
        "  test_seq = np.stack(test_seq, axis=0)\n",
        "  test_int = np.stack(test_int, axis=0)\n",
        "\n",
        "  np.save(\"test_pre.npy\", test_pre)\n",
        "  np.save(\"test_int.npy\", test_int)\n",
        "  np.save(\"test_seq.npy\", test_seq)"
      ],
      "metadata": {
        "id": "kbPWj1ozfhPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create cross-validation splits\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.utils import shuffle\n",
        "np.random.seed(19)\n",
        "\n",
        "def split_cross_val(training):\n",
        "  '''\n",
        "  Splits training df into 5 cross-validation splits and saves them\n",
        "  '''\n",
        "\n",
        "  splits=[]\n",
        "\n",
        "  # shuffle the rows of the training dataframe\n",
        "  training = shuffle(training)\n",
        "\n",
        "  gkf = GroupKFold(n_splits=5)\n",
        "  for train_index, valid_index in gkf.split(training, groups=training['peptide_group']):\n",
        "      train_df = training.iloc[train_index]\n",
        "      valid_df = training.iloc[valid_index]\n",
        "      splits.append((train_df, valid_df))\n",
        "\n",
        "  s_train_list = []\n",
        "  s_valid_list = []\n",
        "\n",
        "  for i in range(5):\n",
        "      s_train = splits[i][0]\n",
        "      s_valid = splits[i][1]\n",
        "      s_train_list.append(s_train)\n",
        "      s_valid_list.append(s_valid)\n",
        "\n",
        "\n",
        "  for i in range(5):\n",
        "    s_train = s_train_list[i]\n",
        "    s_valid = s_valid_list[i]\n",
        "    \n",
        "    s_train_pre = s_train[[\"precursor_charge\"]].to_numpy()\n",
        "    s_train_int = s_train[[\"target\"]].to_numpy()\n",
        "    s_train_seq = s_train[[\"peptide_sequence_encoded\"]].to_numpy()\n",
        "    s_train_seq = np.array(s_train_seq).flatten()\n",
        "    s_train_int = np.array(s_train_int).flatten()\n",
        "    s_train_seq = np.stack(s_train_seq, axis=0)\n",
        "    s_train_int = np.stack(s_train_int, axis=0)\n",
        "    s_train_pre = s_train_pre.flatten()\n",
        "\n",
        "    s_valid_pre = s_valid[[\"precursor_charge\"]].to_numpy()\n",
        "    s_valid_int = s_valid[[\"target\"]].to_numpy()\n",
        "    s_valid_seq = s_valid[[\"peptide_sequence_encoded\"]].to_numpy()\n",
        "    s_valid_seq = np.array(s_valid_seq).flatten()\n",
        "    s_valid_int = np.array(s_valid_int).flatten()\n",
        "    s_valid_seq = np.stack(s_valid_seq, axis=0)\n",
        "    s_valid_int = np.stack(s_valid_int, axis=0)\n",
        "    s_valid_pre = s_valid_pre.flatten()\n",
        "\n",
        "    np.save(f\"s{i}_train_pre.npy\", s_train_pre)\n",
        "    np.save(f\"s{i}_train_int.npy\", s_train_int)\n",
        "    np.save(f\"s{i}_train_seq.npy\", s_train_seq)\n",
        "    np.save(f\"s{i}_valid_pre.npy\", s_valid_pre)\n",
        "    np.save(f\"s{i}_valid_int.npy\", s_valid_int)\n",
        "    np.save(f\"s{i}_valid_seq.npy\", s_valid_seq)"
      ],
      "metadata": {
        "id": "79lozRa9fh3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Building a Model\n",
        "* **GOAL:** Create a model, which \n",
        "  * takes two inputs\n",
        "  * has max 2 convolutional layers\n",
        "  * maximizes spectral angle value"
      ],
      "metadata": {
        "id": "a63OPYNqhBjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spectral_angle_loss(y_true, y_pred):\n",
        "  import keras.backend as k\n",
        "  import tensorflow\n",
        "  # Normalize the vectors\n",
        "  x = k.l2_normalize(y_true, axis=-1)\n",
        "  y = k.l2_normalize(y_pred, axis=-1)\n",
        "\n",
        "  # Calculate the dot product between the vectors\n",
        "  dot_product = k.sum(x * y, axis=-1)\n",
        "\n",
        "  # Return the spectral angle\n",
        "  return -(1 - 2 * tensorflow.acos(dot_product) / np.pi )"
      ],
      "metadata": {
        "id": "3Gsf81idhK2C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1\n",
        "* takes two one-hot encoded inputs:\n",
        "    * input A: precursor charge\n",
        "    * input B: peptide sequence\n",
        "* outputs 56-dimensional tensor where first 28 elements are b ions, last 28 elements y ions"
      ],
      "metadata": {
        "id": "HmWFNQSvllnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(50, 7, activation='relu')(seq_input)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, x], axis=-1)\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model1')"
      ],
      "metadata": {
        "id": "WS2VTvDhleeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 7), (None, 24, 64)]"
      ],
      "metadata": {
        "id": "0fPd0eAYlzR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2\n",
        "\n",
        "- added Flatten layer to fix the error"
      ],
      "metadata": {
        "id": "Uc5B3pxJm9TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(50, 7, activation='relu')(seq_input)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model2')"
      ],
      "metadata": {
        "id": "wX3ESYsCmBru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3\n",
        "\n",
        "- added another Convolutional layer"
      ],
      "metadata": {
        "id": "NIrWIwDxm5CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(50, 7, activation='relu')(seq_input)\n",
        "  x = layers.Conv1D(300, 7, activation='relu')(x)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model3')"
      ],
      "metadata": {
        "id": "OSPkyOlRmuXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 4\n",
        "\n",
        "* added MaxPooling1D layer"
      ],
      "metadata": {
        "id": "Ga9Y8tHOnCs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(50, 7, activation='relu')(seq_input)\n",
        "  x = layers.Conv1D(300, 7, activation='relu')(x)\n",
        "  x = layers.MaxPooling1D()(x)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model4')"
      ],
      "metadata": {
        "id": "UQEaTDHZnZno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 5\n",
        "\n",
        "* added BatchNormalization layers"
      ],
      "metadata": {
        "id": "L3IkbcyFnlIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(64, 7, activation='relu')(seq_input)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPooling1D()(x)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model_5')"
      ],
      "metadata": {
        "id": "x4h0ptCNnvva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning using Weights&Biases Framework\n",
        "\n",
        "https://api.wandb.ai/links/annah07/yhl8pw2v"
      ],
      "metadata": {
        "id": "Fl4oauUgjtej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model Architecture\n",
        "<img src = 'https://drive.google.com/uc?id=1my1hVvG5nDUJJmoDMPlNaKn1aDxKK0e5' width=700>"
      ],
      "metadata": {
        "id": "QB5e37n9pBq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras import optimizers \n",
        "\n",
        "def create_model():\n",
        "\n",
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(64, 7, activation='relu')(seq_input)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPooling1D()(x)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(128, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='final_model')\n",
        "\n",
        "  return model\n",
        "\n",
        "def compile_model(model):\n",
        "  model.compile(\n",
        "      optimizer = optimizers.Adam(),\n",
        "      loss = spectral_angle_loss,\n",
        "      metrics = ['cosine_similarity', spectral_angle_loss])\n",
        "\n",
        "def load_and_fit (model, num_epochs=5, batch_size=128):\n",
        "  for i in range(0, 5):\n",
        "      train_int = np.load(f\"/content/drive/MyDrive/splits/split{i}/s{i}_train_int.npy\") \n",
        "      train_pre = np.load(f\"/content/drive/MyDrive/splits/split{i}/s{i}_train_pre.npy\")\n",
        "      train_seq = np.load(f\"/content/drive/MyDrive/splits/split{i}/s{i}_train_seq.npy\")\n",
        "      valid_int = np.load(f\"/content/drive/MyDrive/splits/split{i}/s{i}_valid_int.npy\")\n",
        "      valid_pre = np.load(f\"/content/drive/MyDrive/splits/split{i}/s{i}_valid_pre.npy\")\n",
        "      valid_seq = np.load(f\"/content/drive/MyDrive/splits/split{i}/s{i}_valid_seq.npy\")\n",
        "\n",
        "      # One-hot encode the peptide sequences and the intensities\n",
        "      train_seq = tf.one_hot(train_seq, 22)\n",
        "      train_pre = tf.one_hot(train_pre, 7)\n",
        "      valid_seq = tf.one_hot(valid_seq, 22)\n",
        "      valid_pre = tf.one_hot(valid_pre, 7)\n",
        "\n",
        "      # Train the model on the current cross-validation set\n",
        "      model.fit(x=[train_pre, train_seq], \n",
        "                y=train_int,\n",
        "                epochs=num_epochs, \n",
        "                batch_size=batch_size,\n",
        "                validation_data=([valid_pre, valid_seq], [valid_int]))\n",
        "\n",
        "      # Free the memory by deleting the one-hot encoded data\n",
        "      del train_seq, train_pre, train_int, valid_seq, valid_pre, valid_int\n",
        "      tf.keras.backend.clear_session()\n",
        "  return model"
      ],
      "metadata": {
        "id": "Ie2jjuf7jQjg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# Load Holdout Set\n",
        "# Input \n",
        "hold_pre = np.load('/content/drive/MyDrive/splits/holdout/test_pre.npy')\n",
        "hold_seq = np.load('/content/drive/MyDrive/splits/holdout/test_seq.npy')\n",
        "hold_int = np.load('/content/drive/MyDrive/splits/holdout/test_int.npy')\n",
        "\n",
        "# One-Hot-Encoding\n",
        "# Sequence\n",
        "X_hold_seq = tf.one_hot(hold_seq, depth=22)\n",
        "\n",
        "# Precursor Charge\n",
        "X_hold_pre = tf.one_hot(hold_pre, depth=7)"
      ],
      "metadata": {
        "id": "veJq2PPYp6Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict([X_hold_pre, X_hold_seq])"
      ],
      "metadata": {
        "id": "KsmRCwnWqFtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**"
      ],
      "metadata": {
        "id": "u4NOqmk3qOG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1XrFmmg6TL93okL03sY3r_i3s21T8XQK9' width=500>"
      ],
      "metadata": {
        "id": "vKLmnVdE20V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "def spectral_angle(x, y):\n",
        "  '''\n",
        "  Calculates and returns a single spectral angle value\n",
        "  '''\n",
        "  x_norm = np.linalg.norm(x)\n",
        "  y_norm = np.linalg.norm(y)\n",
        "  prod = np.dot(x/x_norm, y/y_norm)\n",
        "  if prod > 1.0:\n",
        "    prod = 1\n",
        "  return 1-2*(np.arccos(prod)/np.pi)\n",
        "\n",
        "def calculate_spectral_angle(reference, holdout):\n",
        "  '''\n",
        "  Returns a numpy array of spectral angle values for reference and holdout\n",
        "  '''\n",
        "  spectral_angle_vals = []\n",
        "  for i in range(holdout.shape[0]):\n",
        "    holdout_value = holdout[i,:]\n",
        "    reference_value = reference[i,:]\n",
        "    spectral_angle_value = spectral_angle(holdout_value, reference_value)\n",
        "    spectral_angle_vals.append(spectral_angle_value)\n",
        "  return np.array(spectral_angle_vals)\n",
        "\n",
        "def cosine_similarity(x, y):\n",
        "  '''\n",
        "  Calculates and returns a single cosine similarity value for x and y\n",
        "  '''\n",
        "  return 1 - spatial.distance.cosine(x, y)\n",
        "\n",
        "def calculate_cosine_similarity(reference, holdout):\n",
        "  '''\n",
        "  Returns a numpy array of cosine similarity values for reference and holdout\n",
        "  '''\n",
        "  cosine_similarities = []\n",
        "  for i in range(holdout.shape[0]):\n",
        "    holdout_value = holdout[i,:]\n",
        "    reference_value = reference[i,:]\n",
        "    cosine_similarity_value = cosine_similarity(holdout_value, reference_value)\n",
        "    cosine_similarities.append(cosine_similarity_value)\n",
        "  return np.array(cosine_similarities)"
      ],
      "metadata": {
        "id": "1LjNvM8BqRVM"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}
