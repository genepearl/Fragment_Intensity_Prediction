{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgSkIbQuWDXVDqpAe9FYh9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veni-vidi-vici-tech/Fragment_Intensity_Prediction/blob/main/Fragment_Intensity_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fragment Intensity Prediction"
      ],
      "metadata": {
        "id": "RvCP8daVK4ir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GOAL**: Develop a machine learning model to predict Fragment ion intensities of y and b singly charged ions.\n",
        "\n",
        "Example:\n",
        "* Input: </br>\n",
        "Peptide Sequence: LTQETNRV, Precursor Charge: 4 \n",
        "* Output: \n",
        "\n",
        "  <img src='pictures_and_graphs/desired_output.png' width=600>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oykTuUbeK_as"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Data Analysis"
      ],
      "metadata": {
        "id": "08_7G9YAQnUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 74,817,546 initial entries\n",
        "\n",
        "* 7 columns with no missing values:\n",
        "\n",
        "  <img src='pictures_and_graphs/data_analysis.png' width=200>\n",
        "\n",
        "\n",
        "###peptide_sequence column\n",
        "\n",
        "* 393.369 unique sequences\n",
        "* 7-30 amino acid length range\n",
        "<img src='pictures_and_graphs/average_peptide_length.png' width=500>\n",
        "\n",
        "* max number of occurances: 31.516 (HWYITTGPVREK)\n",
        "* min number of occurances: 7 (QQQQQQQQQQQQRR)\n",
        "* average number of occurances per sequence: 190\n",
        "\n",
        "* There are two types of UNIMOD modifications: C[UNIMOD:4] and M[UNIMOD:35]\n",
        "* Cytosine is *always* UNIMOD:4 modified, while Methionine can be UNIMOD:35 modified or not\n",
        "\n",
        "###raw file and scan_number columns\n",
        "* One spectrum: raw_file and scan_number stay the same\n",
        "* **UNIQUE IDENTIFIER: peptide_sequence + raw_file + scan_number**\n",
        "\n",
        "###precursor charge\n",
        "\n",
        "  <img src='pictures_and_graphs/precursor_charge.png' width=200>\n",
        "\n",
        "* The most common precursor_charge values are 2 and 3\n",
        "* Some sequences have several different precursor_charge values\n",
        "\n",
        "###ion type and no columns\n",
        "* There are two types of ions: b and y.\n",
        "* There are more y ions (37,404,506) than b ions (36,745,591)\n",
        "* 1-28 ion no range\n",
        "\n",
        "###intensity\n",
        "<img src='pictures_and_graphs/avarage_intensity.png' width=600>\n",
        "\n",
        "* values need to be normalized\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QDwX6gcEQpzV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Data Preprocessing"
      ],
      "metadata": {
        "id": "AfI9H7o6Mi3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Removing Confusing Values\n",
        "\n",
        "- Data was merged from several files\n",
        "- There are 74,817,546 entries in our dataset at the beginning\n",
        "- **GOAL**: remove errors and noise from the data"
      ],
      "metadata": {
        "id": "nktxjbp5ZV_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(df):\n",
        "  ''' \n",
        "  Checks the DataFrame for errors and noise and \n",
        "  returns it without them\n",
        "  '''\n",
        "  # 2.1 Removing join errors\n",
        "  df['precursor_max'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file'])['precursor_charge'].transform(max)\n",
        "  df['precursor_min'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file'])['precursor_charge'].transform(min)\n",
        "  df.loc[df['precursor_max'] != df['precursor_min']]\n",
        "  df.drop(df.index[df['precursor_max'] != df['precursor_min']], inplace = True)\n",
        "  df.drop(['precursor_max', 'precursor_min'], axis=1, inplace=True)\n",
        "\n",
        "  # 2.2 Removing noise from the data\n",
        "  df.drop(df.index[df[\"peptide_sequence\"].str.len() == df['no']], inplace = True)\n",
        "  return df"
      ],
      "metadata": {
        "id": "A9W6DtRIZwcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After applying clean_data function 667958 rows were removed from the dataset"
      ],
      "metadata": {
        "id": "K9x6cy5YZ6ey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Peptide_Sequence Column Cleaning\n",
        "\n",
        "* `peptide_sequence` column contains UNIMOD modifications:\n",
        "  *  `C[UNIMOD:4]` = cystein is **always modified** (in carboxyamidomethylated form)\n",
        "  *  `M[UNIMOD:35]` = methionine is **in some cases modified** (oxidized to methionine sulfoxide) \n",
        "\n",
        "* **GOAL**: make the data in 'peptide_sequence' column consistent by removing [UNIMOD:4] and replacing [UNIMOD:35] with 'O'"
      ],
      "metadata": {
        "id": "390gU2B6Z5Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['peptide_sequence'] = df['peptide_sequence'].apply(lambda x: x.replace('[UNIMOD:4','')\n",
        "                                                                 .replace(']','')\n",
        "                                                                 .replace('M[UNIMOD:35', 'O'))"
      ],
      "metadata": {
        "id": "Cw1ZBSgwa-F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exploring intensities of neighbouring amino acids"
      ],
      "metadata": {
        "id": "Vb8ZtZWsitHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The `b and y ions` for a given peptide represent the two halves formed by splitting the original peptide at a **peptide bond** between two amino acids.\n",
        "\n",
        "<div>\n",
        "<img src=\"pictures_and_graphs/b_and_y_ions.JPG\" width=\"600\"/>\n",
        "</div>"
      ],
      "metadata": {
        "id": "55wt4SQmhwg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='pictures_and_graphs/neighbouring_aas.png'>"
      ],
      "metadata": {
        "id": "M-gnugediLvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Proline** at *C-Terminus* tends to have high intensity with almost all neighboring amino acids\n",
        "* **Arginine** tends to have have intensity with neighboring Arginine"
      ],
      "metadata": {
        "id": "QKuXlxg_huMd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Normalization\n",
        "\n",
        "- Big fluctuations between `intensity` values \n",
        "- **GOAL**: normalize `intensity` column values (values between 0.0 and 1.0)"
      ],
      "metadata": {
        "id": "qP-2BmSDbMdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_intensity(df):\n",
        "  '''\n",
        "  Takes intensity values from the DataFrame, normalizes them using \n",
        "  cumulative intensity normalization method and returns the DataFrame \n",
        "  with the new normalized values in a ['normalized_intensity'] column\n",
        "  '''\n",
        "  # 1. Assigning rank to every intensity in the spectra\n",
        "  df['rank'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file'])[\"intensity\"].rank(\"dense\", ascending=False)\n",
        "\n",
        "  # 2. Calculating intensity total within each spectra\n",
        "  df['total'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file'])[\"intensity\"].transform('sum')\n",
        "\n",
        "  # 3. Sorting values within the spectra based on their rank\n",
        "  df['sum_greater_than'] = df.sort_values('rank', ascending=False).groupby(['peptide_sequence', 'scan_number', 'raw_file'])['intensity'].cumsum()\n",
        "\n",
        "  # 4. Calculating the normalized intensity values within the spectra\n",
        "  df['normalized_intensity'] = df['sum_greater_than']/df['total']\n",
        "\n",
        "  # 5. Dropping helper-columns\n",
        "  df.drop(['rank', 'sum_greater_than', 'total'], axis = 1, inplace = True) \n",
        "  df\n",
        "  return df"
      ],
      "metadata": {
        "id": "mby5fZOib4fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='pictures_and_graphs/norm_intensity.png' width=600>"
      ],
      "metadata": {
        "id": "DVVSBIx-b9Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Turning data into numbers\n",
        "\n",
        "* `ion_type`, `no` and `intensity` are target features and need to be part of the same NumPy array structured like so:\n",
        "\n",
        "  * index 0-27 => ion type b 1-28\n",
        "  * index 28-55 => ion type y 1-28\n",
        "\n",
        "* `peptide_sequence` column needs to be converted into numbers using dictionary:\n",
        "\n",
        "  <img src=\"pictures_and_graphs/amino_acid_key.png\" \n",
        "  width=\"100\" /> \n"
      ],
      "metadata": {
        "id": "XFYMMKNAcR59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def create_target():\n",
        "  df['group'] = df.groupby(['peptide_sequence', 'scan_number', 'raw_file']).ngroup()\n",
        "  # df['group'] = df.groupby(['peptide_sequence', 'precursor_charge']).ngroup() \n",
        "  groups = df.groupby('group')\n",
        "  result = []\n",
        "  for name, group in groups:\n",
        "    intensities = np.zeros(56)\n",
        "    ion_groups = group.groupby('ion_type')\n",
        "\n",
        "    for ion_name, ion_group in ion_groups:\n",
        "      if ion_name == 'b':\n",
        "        indices = ion_group['no'] - 1\n",
        "        #intensities[indices] = ion_group['mean_normalized_intensity'].values\n",
        "        intensities[indices] = ion_group['normalized_intensity'].values\n",
        "      else:\n",
        "        indices = 27 + ion_group['no']\n",
        "        #intensities[indices] = ion_group['mean_normalized_intensity'].values\n",
        "        intensities[indices] = ion_group['normalized_intensity'].values\n",
        "    result.append(intensities)"
      ],
      "metadata": {
        "id": "7qxneXHic8PF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 'peptide_length' column for further use\n",
        "df['peptide_length'] = df['peptide_sequence'].apply(len)"
      ],
      "metadata": {
        "id": "pUYbU9xHfHT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_peptides(df):\n",
        "\n",
        "  # 1. Make sure that all the sequences have the same length\n",
        "  # Finding the longest sequence\n",
        "  max_len = df['peptide_sequence'].str.len().max()\n",
        "  # Addding dummy values to shorter sequences to achieve the same length for every sequence\n",
        "  df['peptide_sequence_encoded'] = df['peptide_sequence'].apply(lambda x: x + 'X'*(max_len - len(x)))\n",
        "  # 2. Separate all the letters with comma\n",
        "  df['peptide_sequence_encoded'] = df['peptide_sequence_encoded'].agg(lambda x: ','.join(x))\n",
        "  # 3. Replace one-letter code with numbers\n",
        "  # Creating a dictionary, where dummy value X, every amino acid and 'O'(oxidized methionine) is numbered\n",
        "  aa_dict = {'X' : '0',\n",
        "             'A' : '1', 'C' : '2', 'D' : '3',\n",
        "             'E' : '4', 'F' : '5', 'G' : '6', \n",
        "             'H' : '7', 'I' : '8', 'K' : '9', \n",
        "             'L' : '10', 'M' : '11', 'N' : '12', \n",
        "             'P' : '13', 'Q' : '14', 'R' : '15',\n",
        "             'S' : '16', 'T' : '17', 'V' : '18',\n",
        "             'W' : '19', 'Y' : '20', 'O' : '21'}\n",
        "  # Iterating over all key-value pairs in aa_dict dictionary\n",
        "  for key, value in aa_dict.items():\n",
        "      # Replace key character(one letter code) with value character(number) in a sequence\n",
        "      df['peptide_sequence_encoded'] = df['peptide_sequence_encoded'].apply(lambda x: x.replace(key, value))\n",
        "\n",
        "  # 4. Turn string of numbers into integers\n",
        "  df['peptide_sequence_encoded'] = df['peptide_sequence_encoded'].apply(lambda x: [int(i) for i in x.split(\",\")])"
      ],
      "metadata": {
        "id": "aK5eG2CAdUR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Splitting the data\n",
        "\n",
        "* Some `peptide_sequence` values have several `precursor charge` values and need to be in the same split\n",
        "* **GOAL**: \n",
        "  * data split into 20% test_set + 80% training\n",
        "  * training further split into 5 cross-validation splits: 80% train + 20% validation\n",
        "  "
      ],
      "metadata": {
        "id": "S-6NNyhwdcXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "np.random.seed(19)\n",
        "def split_train_test(df, test_size):\n",
        "  '''\n",
        "  Splits the df into train and test splits\n",
        "  '''\n",
        "\n",
        "  # Create a new column in your dataframe that groups rows with the same peptide sequence value\n",
        "  df['peptide_group'] = df.groupby('peptide_sequence').ngroup()\n",
        "\n",
        "  # Create an instance of GroupShuffleSplit\n",
        "  gss = GroupShuffleSplit(n_splits=1, test_size=test_size)\n",
        "\n",
        "  # Split your dataframe into train and test sets using the iterator\n",
        "  for train_index, test_index in gss.split(df, groups=df['peptide_group']):\n",
        "      training = df.iloc[train_index]\n",
        "      test_df = df.iloc[test_index]\n",
        "\n",
        "  return training, test_df"
      ],
      "metadata": {
        "id": "FjXznECZeyQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_common_peptides(df_1, df_2):\n",
        "  '''\n",
        "  Finds common peptides between df_1 and df_2\n",
        "  '''\n",
        "  df_1_peptides = set(df_1['peptide_sequence'])\n",
        "  df_2_peptides = set(df_2['peptide_sequence'])\n",
        "\n",
        "  common_peptides = df_1_peptides & df_2_peptides\n",
        "  print(common_peptides)"
      ],
      "metadata": {
        "id": "xQEodk94foHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_precursor_percentage(splitted_df, original_df):\n",
        "  '''\n",
        "  Calculates the percentage of precursor charge values in the train split\n",
        "  '''\n",
        "  precursor_charge_counts = splitted_df['precursor_charge'].value_counts()\n",
        "  return ((precursor_charge_counts / original_df['precursor_charge'].value_counts()) * 100)\n",
        "\n",
        "\n",
        "def calculate_peptide_length_percentage(splitted_df, original_df):\n",
        "  '''\n",
        "  Calculates the percentage of precursor charge values in the train split\n",
        "  '''\n",
        "  peptide_length_counts = splitted_df['peptide_length'].value_counts()\n",
        "  return ((peptide_length_counts / original_df['peptide_length'].value_counts()) * 100)"
      ],
      "metadata": {
        "id": "o5w4g5cVfCag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_test(test_df):\n",
        "  '''\n",
        "  Transforms test_df into right format \n",
        "  and saves it as 3 NumPy arrays\n",
        "  '''\n",
        "  # Transforming data into numpy array\n",
        "  test_pre = test_df[[\"precursor_charge\"]].to_numpy()\n",
        "  test_int = test_df[[\"target\"]].to_numpy()\n",
        "  test_seq = test_df[[\"peptide_sequence_encoded\"]].to_numpy()\n",
        "\n",
        "  test_seq = np.array(test_seq).flatten()\n",
        "  test_int = np.array(test_int).flatten()\n",
        "  test_pre = test_pre.flatten()\n",
        "\n",
        "  test_seq = np.stack(test_seq, axis=0)\n",
        "  test_int = np.stack(test_int, axis=0)\n",
        "\n",
        "  np.save(\"test_pre.npy\", test_pre)\n",
        "  np.save(\"test_int.npy\", test_int)\n",
        "  np.save(\"test_seq.npy\", test_seq)"
      ],
      "metadata": {
        "id": "kbPWj1ozfhPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create cross-validation splits\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.utils import shuffle\n",
        "np.random.seed(19)\n",
        "\n",
        "def split_cross_val(training):\n",
        "  '''\n",
        "  Splits training df into 5 cross-validation splits and saves them\n",
        "  '''\n",
        "\n",
        "  splits=[]\n",
        "\n",
        "  # shuffle the rows of the training dataframe\n",
        "  training = shuffle(training)\n",
        "\n",
        "  gkf = GroupKFold(n_splits=5)\n",
        "  for train_index, valid_index in gkf.split(training, groups=training['peptide_group']):\n",
        "      train_df = training.iloc[train_index]\n",
        "      valid_df = training.iloc[valid_index]\n",
        "      splits.append((train_df, valid_df))\n",
        "\n",
        "  s_train_list = []\n",
        "  s_valid_list = []\n",
        "\n",
        "  for i in range(5):\n",
        "      s_train = splits[i][0]\n",
        "      s_valid = splits[i][1]\n",
        "      s_train_list.append(s_train)\n",
        "      s_valid_list.append(s_valid)\n",
        "\n",
        "\n",
        "  for i in range(5):\n",
        "    s_train = s_train_list[i]\n",
        "    s_valid = s_valid_list[i]\n",
        "    \n",
        "    s_train_pre = s_train[[\"precursor_charge\"]].to_numpy()\n",
        "    s_train_int = s_train[[\"target\"]].to_numpy()\n",
        "    s_train_seq = s_train[[\"peptide_sequence_encoded\"]].to_numpy()\n",
        "    s_train_seq = np.array(s_train_seq).flatten()\n",
        "    s_train_int = np.array(s_train_int).flatten()\n",
        "    s_train_seq = np.stack(s_train_seq, axis=0)\n",
        "    s_train_int = np.stack(s_train_int, axis=0)\n",
        "    s_train_pre = s_train_pre.flatten()\n",
        "\n",
        "    s_valid_pre = s_valid[[\"precursor_charge\"]].to_numpy()\n",
        "    s_valid_int = s_valid[[\"target\"]].to_numpy()\n",
        "    s_valid_seq = s_valid[[\"peptide_sequence_encoded\"]].to_numpy()\n",
        "    s_valid_seq = np.array(s_valid_seq).flatten()\n",
        "    s_valid_int = np.array(s_valid_int).flatten()\n",
        "    s_valid_seq = np.stack(s_valid_seq, axis=0)\n",
        "    s_valid_int = np.stack(s_valid_int, axis=0)\n",
        "    s_valid_pre = s_valid_pre.flatten()\n",
        "\n",
        "    np.save(f\"s{i}_train_pre.npy\", s_train_pre)\n",
        "    np.save(f\"s{i}_train_int.npy\", s_train_int)\n",
        "    np.save(f\"s{i}_train_seq.npy\", s_train_seq)\n",
        "    np.save(f\"s{i}_valid_pre.npy\", s_valid_pre)\n",
        "    np.save(f\"s{i}_valid_int.npy\", s_valid_int)\n",
        "    np.save(f\"s{i}_valid_seq.npy\", s_valid_seq)"
      ],
      "metadata": {
        "id": "79lozRa9fh3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Building a Model\n",
        "* **GOAL:** Create a model, which \n",
        "  * takes two inputs\n",
        "  * has max 2 convolutional layers\n",
        "  * maximizes spectral angle value"
      ],
      "metadata": {
        "id": "a63OPYNqhBjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spectral_angle_loss(y_true, y_pred):\n",
        "  import keras.backend as k\n",
        "  import tensorflow\n",
        "  # Normalize the vectors\n",
        "  x = k.l2_normalize(y_true, axis=-1)\n",
        "  y = k.l2_normalize(y_pred, axis=-1)\n",
        "\n",
        "  # Calculate the dot product between the vectors\n",
        "  dot_product = k.sum(x * y, axis=-1)\n",
        "\n",
        "  # Return the spectral angle\n",
        "  return -(1 - 2 * tensorflow.acos(dot_product) / np.pi )"
      ],
      "metadata": {
        "id": "3Gsf81idhK2C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1\n",
        "* takes two one-hot encoded inputs:\n",
        "    * input A: precursor charge\n",
        "    * input B: peptide sequence\n",
        "* outputs 56-dimensional tensor where first 28 elements are b ions, last 28 elements y ions"
      ],
      "metadata": {
        "id": "HmWFNQSvllnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(50, 7, activation='relu')(seq_input)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, x], axis=-1)\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model1')"
      ],
      "metadata": {
        "id": "WS2VTvDhleeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ValueError: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 7), (None, 24, 64)]"
      ],
      "metadata": {
        "id": "0fPd0eAYlzR7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2\n",
        "\n",
        "- added Flatten layer to fix the error"
      ],
      "metadata": {
        "id": "Uc5B3pxJm9TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(50, 7, activation='relu')(seq_input)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model2')"
      ],
      "metadata": {
        "id": "wX3ESYsCmBru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3\n",
        "\n",
        "- added another Convolutional layer"
      ],
      "metadata": {
        "id": "NIrWIwDxm5CM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(50, 7, activation='relu')(seq_input)\n",
        "  x = layers.Conv1D(300, 7, activation='relu')(x)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model3')"
      ],
      "metadata": {
        "id": "OSPkyOlRmuXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 4\n",
        "\n",
        "* added MaxPooling1D layer"
      ],
      "metadata": {
        "id": "Ga9Y8tHOnCs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(50, 7, activation='relu')(seq_input)\n",
        "  x = layers.Conv1D(300, 7, activation='relu')(x)\n",
        "  x = layers.MaxPooling1D()(x)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model4')"
      ],
      "metadata": {
        "id": "UQEaTDHZnZno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 5\n",
        "\n",
        "* added BatchNormalization layers"
      ],
      "metadata": {
        "id": "L3IkbcyFnlIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(64, 7, activation='relu')(seq_input)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPooling1D()(x)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(112, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='model_5')"
      ],
      "metadata": {
        "id": "x4h0ptCNnvva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning using Weights&Biases Framework\n",
        "\n",
        "https://api.wandb.ai/links/annah07/yhl8pw2v"
      ],
      "metadata": {
        "id": "Fl4oauUgjtej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final Model Architecture\n",
        "<img src = 'pictures_and_graphs/my_model.jpg' width=700>"
      ],
      "metadata": {
        "id": "QB5e37n9pBq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "from keras import Input\n",
        "from keras.models import Model\n",
        "from keras.utils import to_categorical\n",
        "from keras import optimizers \n",
        "\n",
        "def create_model():\n",
        "\n",
        "  # Adding Input A (precursor charge)\n",
        "  precursor_input = Input(shape=(7,), name='precursor')\n",
        "  dense = layers.Dense(7, activation='relu')(precursor_input)\n",
        "\n",
        "  # Adding Input B (peptide sequence)\n",
        "  seq_input = Input(shape=(30,22), name='sequence')\n",
        "  x = layers.Conv1D(64, 7, activation='relu')(seq_input)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Conv1D(256, 5, activation='relu')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.MaxPooling1D()(x)\n",
        "  flatten = layers.Flatten()(x)\n",
        "\n",
        "  # Concatenate layers\n",
        "  concat = layers.concatenate([dense, flatten], axis=-1)\n",
        "\n",
        "  x = layers.Dense(128, activation='relu')(concat)\n",
        "  output = layers.Dense(56, activation='sigmoid')(x)\n",
        "\n",
        "  model = Model([precursor_input, seq_input], output, name='final_model')\n",
        "\n",
        "  return model\n",
        "\n",
        "def compile_model(model):\n",
        "  model.compile(\n",
        "      optimizer = optimizers.Adam(),\n",
        "      loss = spectral_angle_loss,\n",
        "      metrics = ['cosine_similarity', spectral_angle_loss])\n",
        "\n",
        "def load_and_fit (model, num_epochs=5, batch_size=128):\n",
        "  for i in range(0, 5):\n",
        "      train_int = np.load(f\"/content/drive/MyDrive/daniel_splits/split{i}/s{i}_train_int.npy\") \n",
        "      train_pre = np.load(f\"/content/drive/MyDrive/daniel_splits/split{i}/s{i}_train_pre.npy\")\n",
        "      train_seq = np.load(f\"/content/drive/MyDrive/daniel_splits/split{i}/s{i}_train_seq.npy\")\n",
        "      valid_int = np.load(f\"/content/drive/MyDrive/daniel_splits/split{i}/s{i}_valid_int.npy\")\n",
        "      valid_pre = np.load(f\"/content/drive/MyDrive/daniel_splits/split{i}/s{i}_valid_pre.npy\")\n",
        "      valid_seq = np.load(f\"/content/drive/MyDrive/daniel_splits/split{i}/s{i}_valid_seq.npy\")\n",
        "\n",
        "      # One-hot encode the peptide sequences and the intensities\n",
        "      train_seq = tf.one_hot(train_seq, 22)\n",
        "      train_pre = tf.one_hot(train_pre, 7)\n",
        "      valid_seq = tf.one_hot(valid_seq, 22)\n",
        "      valid_pre = tf.one_hot(valid_pre, 7)\n",
        "\n",
        "      # Train the model on the current cross-validation set\n",
        "      model.fit(x=[train_pre, train_seq], \n",
        "                y=train_int,\n",
        "                epochs=num_epochs, \n",
        "                batch_size=batch_size,\n",
        "                validation_data=([valid_pre, valid_seq], [valid_int]))\n",
        "\n",
        "      # Free the memory by deleting the one-hot encoded data\n",
        "      del train_seq, train_pre, train_int, valid_seq, valid_pre, valid_int\n",
        "      tf.keras.backend.clear_session()\n",
        "  return model"
      ],
      "metadata": {
        "id": "Ie2jjuf7jQjg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def load_and_predict(model):\n",
        "  # Load Holdout Set\n",
        "  # Input \n",
        "  hold_pre = np.load('/content/drive/MyDrive/splits/holdout/test_pre.npy')\n",
        "  hold_seq = np.load('/content/drive/MyDrive/splits/holdout/test_seq.npy')\n",
        "  hold_int = np.load('/content/drive/MyDrive/splits/holdout/test_int.npy')\n",
        "\n",
        "  # One-Hot-Encoding\n",
        "  # Sequence\n",
        "  X_hold_seq = tf.one_hot(hold_seq, depth=22)\n",
        "  # Precursor Charge\n",
        "  X_hold_pre = tf.one_hot(hold_pre, depth=7)\n",
        "  predictions = model.predict([X_hold_pre, X_hold_seq])\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "veJq2PPYp6Tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**"
      ],
      "metadata": {
        "id": "u4NOqmk3qOG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='pictures_and_graphs/performance.png' width=400>"
      ],
      "metadata": {
        "id": "vKLmnVdE20V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "def spectral_angle(x, y):\n",
        "  '''\n",
        "  Calculates and returns a single spectral angle value\n",
        "  '''\n",
        "  x_norm = np.linalg.norm(x)\n",
        "  y_norm = np.linalg.norm(y)\n",
        "  prod = np.dot(x/x_norm, y/y_norm)\n",
        "  if prod > 1.0:\n",
        "    prod = 1\n",
        "  return 1-2*(np.arccos(prod)/np.pi)\n",
        "\n",
        "def calculate_spectral_angle(reference, holdout):\n",
        "  '''\n",
        "  Returns a numpy array of spectral angle values for reference and holdout\n",
        "  '''\n",
        "  spectral_angle_vals = []\n",
        "  for i in range(holdout.shape[0]):\n",
        "    holdout_value = holdout[i,:]\n",
        "    reference_value = reference[i,:]\n",
        "    spectral_angle_value = spectral_angle(holdout_value, reference_value)\n",
        "    spectral_angle_vals.append(spectral_angle_value)\n",
        "  return np.array(spectral_angle_vals)\n",
        "\n",
        "def cosine_similarity(x, y):\n",
        "  '''\n",
        "  Calculates and returns a single cosine similarity value for x and y\n",
        "  '''\n",
        "  return 1 - spatial.distance.cosine(x, y)\n",
        "\n",
        "def calculate_cosine_similarity(reference, holdout):\n",
        "  '''\n",
        "  Returns a numpy array of cosine similarity values for reference and holdout\n",
        "  '''\n",
        "  cosine_similarities = []\n",
        "  for i in range(holdout.shape[0]):\n",
        "    holdout_value = holdout[i,:]\n",
        "    reference_value = reference[i,:]\n",
        "    cosine_similarity_value = cosine_similarity(holdout_value, reference_value)\n",
        "    cosine_similarities.append(cosine_similarity_value)\n",
        "  return np.array(cosine_similarities)"
      ],
      "metadata": {
        "id": "1LjNvM8BqRVM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##IV. Adjustments"
      ],
      "metadata": {
        "id": "aRgsmhYgBwNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Intensity\n",
        "\n",
        "* Our dataset contains rows with the same `peptide_sequence`,  `precursor_charge`, `ion type` and `no` values, but different `intensity` values\n",
        "\n",
        "<img src='pictures_and_graphs/redundancy.JPG'>\n",
        "\n",
        "* **Idea**: calculate mean value for auch `peptide_sequence` + `precursor_charge` + `ion_type` + `no` combination => consequence: smaller dataset for training"
      ],
      "metadata": {
        "id": "as8sdd0dB1rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "THIS WAS DONE BEFORE STEP 4 OF PREPROCESSING ⤵"
      ],
      "metadata": {
        "id": "9EAC13_yHbmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_noise(df):\n",
        "  '''\n",
        "  Takes df with different intensity values for 'petide seq' + 'precursor charge' + \n",
        "  'ion_type' + 'no' combination, calculates mean intensity value for each \n",
        "  combination and returns the preprocessed df with new \n",
        "  'mean_normalized_intensity' column\n",
        "  '''\n",
        "  # Reducing the noise => one normalized intensity value for the same 'petide seq' + 'precursor charge' + 'ion_type' + 'no' combination\n",
        "  result = df.groupby(['peptide_sequence', 'precursor_charge', 'ion_type', 'no'])['normalized_intensity'].mean().reset_index()\n",
        "  result.rename(columns={'normalized_intensity': 'mean_normalized_intensity'})\n",
        "  return result"
      ],
      "metadata": {
        "id": "uQ9vY0AYGs7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* While using create_target() function few adjustments were made (commented out code):\n",
        "  * `group`column: df was grouped by `peptide_sequence` and `precursor_charge`\n",
        "  * `normalized_intensity` was replaced with newly created `mean_normalized_intensity`"
      ],
      "metadata": {
        "id": "MauSD6QjKiyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**"
      ],
      "metadata": {
        "id": "nXI5fVmQLQXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='pictures_and_graphs/mean_intensity_preprocessing_performance.png' width=400>"
      ],
      "metadata": {
        "id": "1EICY7cvKWoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Considering Vs Ignoring Methionine Modification\n",
        "* Methionine was present in two forms with our dataset:\n",
        "  * modified\n",
        "  * unmodified\n",
        "\n",
        "* **Idea**: take the same dataset, divide it into 5 cross-validation sets and a test set. In first case replace methionine and in second don't. Evaluate the unmodified cross-validation splits on unmodified test set & evaluate the modified cross-validation splits on modified test set. Compare performance\n",
        "\n",
        "* Steps 1-3 are the same. Then first the splitting into training and test is done as described in `methionine_train_test_split` function\n",
        "\n",
        "* There are 4 groups present in our dataset ragarding *methionine contents*:",
        "  1. Contains just unmodified methionine\n",
        "  2. Contains just modified methionine\n",
        "  3. Contains both\n",
        "  4. Doesn't contain any methionine\n",
        "\n",
        "\n",
        "* We took 20% of groups 1-3 as a test set. Since group 3 was the smallest in size, we left its test set as is, but we made sure that test sets we got from groups 1 and 2 were equal in size \n"
      ],
      "metadata": {
        "id": "w-x8MCPJB9BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equal_sample(df1, df2):\n",
        "  '''\n",
        "  Compares two dataframes in size, takes a sample of the same len as the smaller \n",
        "  dataframe from the bigger one, returns two dataframes equal in size\n",
        "  '''\n",
        "  if len(df1) > len(df2):\n",
        "    sample_size = len(df2)\n",
        "    sampled_df = df1.sample(n=sample_size, replace=False)\n",
        "  else:\n",
        "    sample_size = len(df1)\n",
        "    sampled_df = df2.sample(n=sample_size, replace=False)\n",
        "  return df1, df2\n",
        "\n",
        "def methionine_train_test_split(df, test_size=0.2):\n",
        "  '''\n",
        "  Splits df into three subsets: group1, group2 and group3. Takes test_size from \n",
        "  each df, makes sure that tests made from group1 and group2 dfs are of equal size,\n",
        "  concatenates three test_sets. That is the new test df, removes 'peptide_sequence' \n",
        "  values from df. That is the training df. Returns test and training\n",
        "  '''\n",
        "  df_unmod = df.loc[(df['peptide_sequence'].str.contains('M')) & \n",
        "                    ~(df['peptide_sequence_modified'].str.contains('O'))]\n",
        "  df_mod = df.loc[(df['peptide_sequence'].str.contains('O')) & \n",
        "                  ~(df['peptide_sequence_modified'].str.contains('M'))]\n",
        "  df_both = df.loc[(df['peptide_sequence'].str.contains('O')) & \n",
        "                   (df['peptide_sequence_modified'].str.contains('M'))]\n",
        "\n",
        "  training_unmod, test_unmod = split_train_test(df_unmod, test_size)\n",
        "  training_mod, test_mod = split_train_test(df_mod, test_size)\n",
        "  test_mod, test_unmod = equal_sample(test_mod, test_unmod)\n",
        "  \n",
        "  training_both, test_both = split_train_test(df_both, test_size)\n",
        "\n",
        "  test = pd.concat([test_mod, test_unmod, test_both])\n",
        "  training = df[~df['peptide_sequence'].isin(test['peptide_sequence'])]\n",
        "  return training, test"
      ],
      "metadata": {
        "id": "-BoRdyrsLWml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* After this the same training and test sets were preprocessed in a different way:\n",
        "  * Considering Modification Splits: encode_peptides, save_test, split_cross_val\n",
        "  * Ignoring Modification Splits: encode_peptides (`aa_dict without '0'`, only 20 letters get replaced with numbers), save_test, split_cross_val "
      ],
      "metadata": {
        "id": "Ub0aBaidd7AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**\n",
        "\n",
        "<img src ='pictures_and_graphs/methionine.png' width=600>"
      ],
      "metadata": {
        "id": "menA3pFghsdT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Proline\n",
        "\n",
        "* has an impact on fragmentation (see heatmap in Step 2 Preprocessing)\n",
        "* **Idea**: train a model on the dataset, which contains both sequences with and without proline and evaluate it on two different holdout sets:\n",
        "    * holdout set with every sequence containing at least one proline\n",
        "    * holdout set with every sequence not containing any proline\n",
        "* all steps 1-4 stay the same, changes in step 5 ⤵"
      ],
      "metadata": {
        "id": "R5qIilvaCCFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_proline_splits(df):\n",
        "  '''\n",
        "  Takes dataframe, sorts out the rows which contain at least one proline (df_proline)\n",
        "  and rows without proline at all (df_no_proline), takes 10% as test set from \n",
        "  each of new subsets, makes sure that training and test sets from each subset \n",
        "  are equal in size, concatenates training sets.\n",
        "  Returns test_proline, test_no_proline and training dataframes\n",
        "  '''\n",
        "  df_proline = df.loc[df['peptide_sequence'].str.contains(\"P\")]\n",
        "  df_no_proline = df.loc[~df['peptide_sequence'].str.contains(\"P\")]\n",
        "\n",
        "  training_proline, test_proline = split_train_test(df_proline, 0.1)\n",
        "  training_no_proline, test_no_proline = split_train_test(df_no_proline, 0.1)\n",
        "\n",
        "  test_proline, test_no_proline = equal_sample(test_proline, test_no_proline)\n",
        "\n",
        "  training_proline, training_no_proline = equal_sample(training_proline, training_no_proline)\n",
        "  training = pd.concat([training_proline, training_no_proline])\n",
        "  return test_proline, test_no_proline, training"
      ],
      "metadata": {
        "id": "pKA2_VvrMLwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESULTS**\n",
        "\n",
        "<img src='pictures_and_graphs/proline.png' width=600>"
      ],
      "metadata": {
        "id": "SP8K2ZL1UzWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two-Sample T-Test proves that results are significantly different and didn't occur by chance\n",
        "\n",
        "<img src='pictures_and_graphs/two_sample_t_test.png' width=450>"
      ],
      "metadata": {
        "id": "Rqz3_bz1Waue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "def two_sample_t_test(predictions1, predictions2):\n",
        "  '''\n",
        "  Takes numpy arrays predictions1 and predictions2, runs two-sample t-test and\n",
        "  prints whether the results are significantly different based on 0.05 threshold\n",
        "  '''\n",
        "  # Conduct the two-sample t-test to compare the results from the two predictive analyses\n",
        "  t_statistic, p_value = stats.ttest_ind(predictions1, predictions2)\n",
        "\n",
        "  # Determine the significance of the difference between the results obtained from the two predictive analyses\n",
        "  if np.all(p_value < 0.05):\n",
        "      print(\"The results obtained are significantly different (p-value = {})\".format(p_value))\n",
        "  else:\n",
        "      print(\"The results obtained are not significantly different (p-value = {})\".format(p_value))"
      ],
      "metadata": {
        "id": "R5Ou8gDsXDAD"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}
